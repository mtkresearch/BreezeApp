# Generate Chat Completion



## 1. Description

Generate the next message in a chat with a provided model.



## 2. Method: `EdgeAI.chat`

An OpenAI-style API for BreezeApp.



## 3. Request Body Parameter Description

| Parameter Name          | Type              | Required | Default | Description                                                                                                                                                                             |
| ----------------------- | ----------------- | -------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`              | array             | Yes      |         | Array of chat messages. Each message must include a role (`user`/`assistant`/`system`) and content. Depending on the model, different modalities (text, image, audio) may be supported. |
| `model`                 | string            | Yes      |         | The model ID to use for generating responses.                                                                                                                                           |
| `frequency_penalty`     | number/null       | No       | 0       | Range: -2.0 ~ 2.0. Positive values penalize tokens that appear frequently in the current text, reducing repetition.                                                                     |
| `logit_bias`            | map               | No       | null    | Adjusts the probability of specific tokens. Format: token ID mapped to bias value (-100~100).                                                                                           |
| `logprobs`              | boolean/null      | No       | false   | Whether to return log probabilities of output tokens. If true, logprobs for each token are included in the message content.                                                             |
| `max_completion_tokens` | integer/null      | No       |         | Limits the maximum number of tokens generated by the model (including visible output and reasoning tokens).                                                                             |
| `metadata`              | map               | No       |         | Up to 16 key-value pairs for custom structured information, useful for query and management. Key max 64 chars, value max 512 chars.                                                     |
| `n`                     | integer/null      | No       | 1       | Number of response options to generate for each input message. Higher n increases cost.                                                                                                 |
| `parallel_tool_calls`   | boolean           | No       | true    | Whether to enable parallel tool (function) calls.                                                                                                                                       |
| `prediction`            | object            | No       |         | Prediction output settings. Can speed up responses when most content is known and only small changes are expected.                                                                      |
| `presence_penalty`      | number/null       | No       | 0       | Range: -2.0 ~ 2.0. Positive values encourage the model to introduce new topics.                                                                                                         |
| `response_format`       | object            | No       |         | Specifies the response format. `{ "type": "json_schema", "json_schema": {...} }` can enforce a JSON schema.                                                                             |
| `seed`                  | integer/null      | No       |         | Experimental. When specified, the system will try to produce the same result for the same seed and parameters (not guaranteed).                                                         |
| `stop`                  | string/array/null | No       | null    | Up to 4 stop sequences. Generation stops when any is encountered. Not supported by o3/o4-mini.                                                                                          |
| `store`                 | boolean/null      | No       | false   | Whether to store this completion for model distillation or evaluation.                                                                                                                  |
| `stream`                | boolean/null      | No       | false   | Whether to return data as a stream.                                                                                                                                                     |
| `stream_options`        | object/null       | No       | null    | Stream response options, only available when `stream: true`.                                                                                                                            |
| `temperature`           | number/null       | No       | 1       | Range: 0~2. Controls randomness; higher values are more random. It is recommended to adjust only temperature or top_p, not both.                                                        |
| `tool_choice`           | string/object     | No       |         | Controls whether the model calls tools. "none" = no call, "auto" = model decides, "required" = must call, or specify a particular tool.                                                 |
| `tools`                 | array             | No       |         | List of tools, currently only supports function. Up to 128 functions.                                                                                                                   |
| `top_logprobs`          | integer/null      | No       |         | 0~20. For each token, returns the top N most likely tokens and their log probabilities. Requires `logprobs: true`.                                                                      |
| `top_p`                 | number/null       | No       | 1       | Range: 0~1. Nucleus sampling; only considers tokens with cumulative probability mass up to top_p. Recommended to adjust only temperature or top_p.                                      |



## 4. Response Format

### 4.1 `stream: false` (Default, Non-Streaming Mode)

The server returns a complete JSON object in a single response after generating the full output.  This is suitable for most synchronous application scenarios.

**Example Response:**

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1715000000,
  "model": "breeze2",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 12,
    "total_tokens": 22
  }
}
```

- `choices`: An array of response options, usually only one (`n=1`).
- `message`: Contains the role and content.
- `usage`: Token usage statistics.



### 4.2 `stream: true` (Streaming Mode)

The server returns response fragments (chunks) incrementally. Each chunk is a JSON object, usually containing only a part of the content (such as a short text segment). The final chunk will have `finish_reason: "stop"` to indicate the end. This is suitable for real-time display, long text, audio, and interactive scenarios.

**Example Streaming Response:**  
The server will return data in the following format multiple times (each line is a chunk, prefixed with `data:`):

```
data: {
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1715000000,
  "model": "breeze2",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",
        "content": "Hello"
      },
      "finish_reason": null
    }
  ]
}

data: {
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1715000000,
  "model": "breeze2",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ]
}

data: [DONE]
```

- `delta`: Each chunk only contains the newly generated content segment.
- `finish_reason`: The last chunk will be marked as `"stop"`.
- `[DONE]`: Indicates the end of the streaming response.



## 5. Example Request Body

```json
{
  "model": "breeze2",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_completion_tokens": 256,
  "n": 1,
  "stream": false,
  "tools": [],
  "tool_choice": null,
  "logit_bias": null,
  "logprobs": false,
  "presence_penalty": 0,
  "frequency_penalty": 0,
  "response_format": null,
  "metadata": {},
  "store": false
}
```

**Explanation:**

- This is a sample request body for the Chat Completions API.
- It specifies the model, a system prompt, a user message, and various generation parameters.
- The request asks for a single, non-streaming response in text format.



## 6. JAVA Examples

### 6.1 Chat Request (No Streaming)

```java
import EdgeAI;
import java.util.*;

public class ChatExampleNoStreaming {
    public static void main(String[] args) {
        // Build the request body as a Map
        Map<String, Object> requestBody = new HashMap<>();

        // Model name
        requestBody.put("model", "breeze2");

        // Chat messages
        List<Map<String, Object>> messages = new ArrayList<>();
        Map<String, Object> message = new HashMap<>();
        message.put("role", "user");
        message.put("content", "Why is the sky blue?");
        messages.add(message);
        requestBody.put("messages", messages);

        // Generation parameters
        requestBody.put("temperature", 0.7);
        requestBody.put("max_completion_tokens", 256);
        requestBody.put("n", 1);
        requestBody.put("stream", false);

        // Optional parameters (can be omitted if not needed)
        requestBody.put("tools", new ArrayList<>());
        requestBody.put("tool_choice", null);
        requestBody.put("logit_bias", null);
        requestBody.put("logprobs", false);
        requestBody.put("presence_penalty", 0);
        requestBody.put("frequency_penalty", 0);
        requestBody.put("response_format", null);
        requestBody.put("metadata", new HashMap<>());
        requestBody.put("store", false);

        // Call EdgeAI.chat method with the request body
        Map<String, Object> response = EdgeAI.chat(requestBody);

        // Handle response
        System.out.println(response);
    }
}
```



### 6.2 Chat Request (Streaming)

```java
import EdgeAI;
import java.util.*;

// Define the callback interface for streaming responses
interface ChatStreamCallback {
    void onChunk(Map<String, Object> chunk);
    void onComplete();
    void onError(Exception e);
}

public class ChatExampleStreaming {
    public static void main(String[] args) {
        // Build the request body as a Map
        Map<String, Object> requestBody = new HashMap<>();

        // Model name
        requestBody.put("model", "breeze2");

        // Chat messages
        List<Map<String, Object>> messages = new ArrayList<>();
        Map<String, Object> message = new HashMap<>();
        message.put("role", "user");
        message.put("content", "Why is the sky blue?");
        messages.add(message);
        requestBody.put("messages", messages);

        // Generation parameters
        requestBody.put("temperature", 0.7);
        requestBody.put("max_completion_tokens", 256);
        requestBody.put("n", 1);
        requestBody.put("stream", true);

        // Optional parameters (can be omitted if not needed)
        requestBody.put("tools", new ArrayList<>());
        requestBody.put("tool_choice", null);
        requestBody.put("logit_bias", null);
        requestBody.put("logprobs", false);
        requestBody.put("presence_penalty", 0);
        requestBody.put("frequency_penalty", 0);
        requestBody.put("response_format", null);
        requestBody.put("metadata", new HashMap<>());
        requestBody.put("store", false);

        // Call EdgeAI.chat method with the request body and a streaming callback
        EdgeAI.chat(requestBody, new ChatStreamCallback() {
            @Override
            public void onChunk(Map<String, Object> chunk) {
                // Handle each streamed chunk (partial response)
                System.out.println("Received chunk: " + chunk);
            }

            @Override
            public void onComplete() {
                // Called when the streaming is finished
                System.out.println("Streaming complete.");
            }

            @Override
            public void onError(Exception e) {
                // Handle any errors during streaming
                e.printStackTrace();
            }
        });
    }
}
```

**Notes:**

- This example assumes the SDK provides a method signature like:  
  `void EdgeAI.chat(Map<String, Object> requestBody, ChatStreamCallback callback);`
- Each chunk received in `onChunk` is a partial response (see your streaming response format).
- `onComplete` is called when the stream ends, and `onError` handles any exceptions.



### 6.3 Chat Request with History (No Streaming)

```java
import EdgeAI;
import java.util.*;

public class ChatExampleWithHistoryNoStreaming {
    public static void main(String[] args) {
        // Build the request body as a Map
        Map<String, Object> requestBody = new HashMap<>();

        // Model name
        requestBody.put("model", "breeze2");

        // Chat messages with history
        List<Map<String, Object>> messages = new ArrayList<>();

        // System prompt
        Map<String, Object> systemMsg = new HashMap<>();
        systemMsg.put("role", "system");
        systemMsg.put("content", "You are a helpful assistant.");
        messages.add(systemMsg);

        // Previous user message
        Map<String, Object> userMsg1 = new HashMap<>();
        userMsg1.put("role", "user");
        userMsg1.put("content", "Why is the sky blue?");
        messages.add(userMsg1);

        // Previous assistant reply
        Map<String, Object> assistantMsg1 = new HashMap<>();
        assistantMsg1.put("role", "assistant");
        assistantMsg1.put("content", "The sky appears blue because of the way Earth's atmosphere scatters sunlight.");
        messages.add(assistantMsg1);

        // New user message
        Map<String, Object> userMsg2 = new HashMap<>();
        userMsg2.put("role", "user");
        userMsg2.put("content", "What about at sunset?");
        messages.add(userMsg2);

        requestBody.put("messages", messages);

        // Generation parameters
        requestBody.put("temperature", 0.7);
        requestBody.put("max_completion_tokens", 256);
        requestBody.put("n", 1);
        requestBody.put("stream", false);

        // Optional parameters
        requestBody.put("tools", new ArrayList<>());
        requestBody.put("tool_choice", null);
        requestBody.put("logit_bias", null);
        requestBody.put("logprobs", false);
        requestBody.put("presence_penalty", 0);
        requestBody.put("frequency_penalty", 0);
        requestBody.put("response_format", null);
        requestBody.put("metadata", new HashMap<>());
        requestBody.put("store", false);

        // Call EdgeAI.chat method with the request body
        Map<String, Object> response = EdgeAI.chat(requestBody);

        // Handle response
        System.out.println(response);
    }
}
```

**Notes:**

- The `messages` list contains the full conversation history, including system, user, and assistant messages.
- The last message is the new user question, and the model will generate a reply based on the entire history.
- The request is non-streaming (`"stream": false`).



## Reference

- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [OpenAI Java API Library](https://github.com/openai/openai-java)



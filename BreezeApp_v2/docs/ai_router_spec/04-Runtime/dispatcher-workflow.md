# ğŸ§µ Dispatcher ä»»å‹™æ´¾ç™¼èˆ‡åŸ·è¡Œç·’ç®¡ç†

## ğŸ¯ è¨­è¨ˆç›®æ¨™

æœ¬æ–‡ä»¶èªªæ˜ BreezeApp AI Router ä¸­æ¨è«–ä»»å‹™çš„å¤šåŸ·è¡Œç·’èˆ‡ Coroutine ç®¡ç†ç­–ç•¥ï¼Œç¢ºä¿é«˜æ•ˆèƒ½ã€éŸ¿æ‡‰å¼çš„ AI æ¨è«–åŸ·è¡Œï¼ŒåŒæ™‚é¿å…é˜»å¡ä¸»åŸ·è¡Œç·’å’Œè³‡æºç«¶çˆ­å•é¡Œã€‚

### æ ¸å¿ƒåŸå‰‡

- **ğŸš« é¿å… UI é˜»å¡**: æ‰€æœ‰æ¨è«–æ“ä½œç§»è‡³èƒŒæ™¯åŸ·è¡Œç·’
- **âš¡ ç•°æ­¥å„ªå…ˆ**: ä½¿ç”¨ Kotlin Coroutines é€²è¡Œç•°æ­¥è™•ç†
- **ğŸ”„ è³‡æºå…±äº«**: æ™ºæ…§ç®¡ç†æ¨¡å‹è¼‰å…¥èˆ‡ session éš”é›¢
- **ğŸ§µ åŸ·è¡Œç·’å®‰å…¨**: ç¢ºä¿ Runner çš„ä¸¦ç™¼å®‰å…¨æ€§
- **ğŸ“Š æ•ˆèƒ½ç›£æ§**: æä¾›åŸ·è¡Œæ™‚é–“èˆ‡è³‡æºä½¿ç”¨è¿½è¹¤

## ğŸ—ºï¸ æ•´é«”åŸ·è¡Œæµç¨‹

### ä¸»è¦åŸ·è¡Œéšæ®µ

```mermaid
flowchart TD
    %% UI Thread
    subgraph UI_Thread["ğŸ¨ UI Thread"]
        A1["ä½¿ç”¨è€…æ“ä½œè§¸ç™¼æ¨è«–è«‹æ±‚"]
        A2["ViewModel å»ºç«‹ InferenceRequest"]
        A1 --> A2
    end

    %% Dispatcher CoroutineScope
    subgraph Dispatcher_Scope["ğŸš€ Dispatcher CoroutineScope"]
        A2 --> B1["Dispatcher æ¥æ”¶è«‹æ±‚"]
        B1 --> B2{"é¸æ“‡ Capability / Runner"}
        B2 --> B3["åˆ‡æ›è‡³ I/O åŸ·è¡Œç·’èª¿åº¦"]
        B2 --> B4["æª¢æŸ¥ä¸¦ç™¼é™åˆ¶èˆ‡å„ªå…ˆç´š"]
    end

    %% IO Thread Pool
    subgraph IO_Thread_Pool["ğŸ’¾ I/O Thread Pool"]
        B3 --> C1["ç¢ºèªæ¨¡å‹æ˜¯å¦è¼‰å…¥"]
        C1 -->|æœªè¼‰å…¥| C2["å‘¼å« ModelManager.load()"]
        C1 -->|å·²è¼‰å…¥| C3["é€è‡³å°æ‡‰ Runner åŸ·è¡Œ"]
        C2 --> C3
    end

    %% Runner åŸ·è¡Œå€
    subgraph Runner_Exec["âš™ï¸ Runner åŸ·è¡Œå€"]
        C3 --> D1["Runner.run() on Worker Thread"]
        D1 --> D2["å‘¼å« RuntimeEngine æ¨è«–"]
        D2 --> D3["ç­‰å¾… JNI å›å‚³çµæœ"]
        D3 --> D4["åŒ…è£çµæœèˆ‡éŒ¯èª¤è™•ç†"]
    end

    %% å›å‚³ä¸»åŸ·è¡Œç·’
    subgraph Callback_Section["ğŸ“± å›å‚³è™•ç†"]
        D4 --> E1["åˆ‡å› MainThread"]
        E1 --> E2["æ›´æ–° UI æˆ–ç‹€æ…‹"]
        E1 --> E3["è§¸ç™¼ Callback"]
    end

    %% éŒ¯èª¤è™•ç†
    subgraph Error_Handling["ğŸš¨ éŒ¯èª¤è™•ç†"]
        D2 -.->|éŒ¯èª¤| F1["åˆ¤æ–·æ˜¯å¦å¯é‡è©¦"]
        F1 -->|å¯é‡è©¦| B2
        F1 -->|ä¸å¯é‡è©¦| F2["åŸ·è¡Œ Fallback ç­–ç•¥"]
        F2 --> E1
    end
```

## ğŸ”§ Kotlin Coroutine å¯¦ä½œ

### æ ¸å¿ƒ Dispatcher æ¶æ§‹

```kotlin
class RequestDispatcher(
    private val modelManager: ModelManager,
    private val runnerRegistry: RunnerRegistry,
    private val fallbackPolicy: FallbackPolicy
) {
    // ä½¿ç”¨å°ˆç”¨çš„ CoroutineScope é€²è¡Œä»»å‹™ç®¡ç†
    private val dispatcherScope = CoroutineScope(
        SupervisorJob() + Dispatchers.Default + CoroutineName("AIRouterDispatcher")
    )
    
    // ä¸¦ç™¼é™åˆ¶ç®¡ç†
    private val concurrentLimiter = Semaphore(MAX_CONCURRENT_REQUESTS)
    private val modelLoadingSemaphore = Semaphore(MAX_CONCURRENT_MODEL_LOADING)
    
    fun dispatchRequest(
        request: InferenceRequest,
        onResult: (InferenceResult) -> Unit,
        onError: (Throwable) -> Unit
    ): Job {
        return dispatcherScope.launch {
            try {
                concurrentLimiter.withPermit {
                    val result = executeRequest(request)
                    withContext(Dispatchers.Main) {
                        onResult(result)
                    }
                }
            } catch (e: Exception) {
                Timber.e(e, "Failed to execute request: ${request.sessionId}")
                withContext(Dispatchers.Main) {
                    onError(e)
                }
            }
        }
    }
    
    private suspend fun executeRequest(request: InferenceRequest): InferenceResult {
        return withContext(Dispatchers.IO) {
            val runner = selectOptimalRunner(request)
            
            // ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
            modelLoadingSemaphore.withPermit {
                ensureModelLoaded(runner, request)
            }
            
            // åŸ·è¡Œæ¨è«–
            executeWithRunner(runner, request)
        }
    }
    
    companion object {
        private const val MAX_CONCURRENT_REQUESTS = 4
        private const val MAX_CONCURRENT_MODEL_LOADING = 2
    }
}
```

### æ™ºæ…§ Runner é¸æ“‡

```kotlin
private suspend fun selectOptimalRunner(request: InferenceRequest): RunnerSpec {
    val candidates = runnerRegistry.getAvailableRunners(request.capability)
        .filter { it.isCompatible(deviceInfo) }
        .filter { it.canHandleRequest(request) }
    
    // æ ¹æ“šç›®å‰è² è¼‰é¸æ“‡æœ€ä½³ Runner
    return candidates
        .sortedWith(compareBy<RunnerSpec> { 
            getCurrentLoad(it.name) 
        }.thenByDescending { 
            it.priority 
        })
        .firstOrNull() 
        ?: throw NoSuitableRunnerException(request.capability)
}

private fun getCurrentLoad(runnerName: String): Float {
    return runnerLoadTracker.getCurrentLoad(runnerName)
}
```

### æ¨¡å‹è¼‰å…¥ç®¡ç†

```kotlin
private suspend fun ensureModelLoaded(
    runnerSpec: RunnerSpec, 
    request: InferenceRequest
): BaseRunner {
    val cacheKey = "${runnerSpec.name}:${request.modelKey}"
    
    return runnerCache.get(cacheKey) ?: run {
        Timber.d("Loading model for runner: ${runnerSpec.name}")
        
        val config = modelManager.getModelConfig(request.modelKey)
        val runner = runnerRegistry.createRunner(runnerSpec.name)
        
        // æ¨¡å‹è¼‰å…¥å¯èƒ½è€—æ™‚ï¼Œä½¿ç”¨ I/O dispatcher
        withContext(Dispatchers.IO) {
            val loadSuccess = runner.load(config)
            if (!loadSuccess) {
                throw ModelLoadException("Failed to load model: ${request.modelKey}")
            }
        }
        
        runnerCache.put(cacheKey, runner)
        runner
    }
}
```

## ğŸ§© åŸ·è¡Œç·’å®‰å…¨ç­–ç•¥

### Runner åŸ·è¡Œç·’å®‰å…¨è™•ç†

```kotlin
class ThreadSafeRunnerExecutor {
    private val runnerLocks = mutableMapOf<String, Mutex>()
    
    suspend fun executeWithRunner(
        runner: BaseRunner,
        request: InferenceRequest
    ): InferenceResult {
        return if (runner.isThreadSafe()) {
            // Thread-safe runner å¯ä»¥ç›´æ¥åŸ·è¡Œ
            runner.run(request)
        } else {
            // é thread-safe runner éœ€è¦åŒæ­¥åŸ·è¡Œ
            val lock = runnerLocks.getOrPut(runner.name) { Mutex() }
            lock.withLock {
                runner.run(request)
            }
        }
    }
}
```

### åŸ·è¡Œç·’æ± é…ç½®

```kotlin
object DispatcherConfiguration {
    // é‡å°ä¸åŒé¡å‹çš„æ“ä½œä½¿ç”¨ä¸åŒçš„åŸ·è¡Œç·’æ± 
    val ModelLoadingDispatcher = Dispatchers.IO.limitedParallelism(2)
    val InferenceDispatcher = Dispatchers.Default.limitedParallelism(4)
    val FileIODispatcher = Dispatchers.IO.limitedParallelism(3)
    
    // è‡ªå®šç¾©åŸ·è¡Œç·’æ± ç”¨æ–¼ CPU å¯†é›†çš„æ¨è«–ä»»å‹™
    val CPUIntensiveDispatcher = Executors.newFixedThreadPool(
        max(1, Runtime.getRuntime().availableProcessors() - 1)
    ).asCoroutineDispatcher()
}
```

## ğŸ“Š ä¸¦ç™¼æ§åˆ¶èˆ‡è³‡æºç®¡ç†

### ä¸¦ç™¼é™åˆ¶ç­–ç•¥

```kotlin
class ConcurrencyManager {
    // å…¨åŸŸä¸¦ç™¼é™åˆ¶
    private val globalSemaphore = Semaphore(8)
    
    // å„èƒ½åŠ›å°ˆç”¨çš„ä¸¦ç™¼é™åˆ¶
    private val capabilitySemaphores = mapOf(
        CapabilityType.LLM to Semaphore(3),
        CapabilityType.ASR to Semaphore(2),
        CapabilityType.TTS to Semaphore(2),
        CapabilityType.VLM to Semaphore(1)  // VLM æ¯”è¼ƒè€—è³‡æº
    )
    
    suspend fun <T> executeWithLimit(
        capability: CapabilityType,
        operation: suspend () -> T
    ): T {
        return globalSemaphore.withPermit {
            capabilitySemaphores[capability]?.withPermit {
                operation()
            } ?: operation()
        }
    }
}
```

### è¨˜æ†¶é«”å£“åŠ›ç›£æ§

```kotlin
class MemoryPressureMonitor {
    private val memoryThreshold = 0.8f // 80% è¨˜æ†¶é«”ä½¿ç”¨ç‡è­¦æˆ’ç·š
    
    fun shouldRejectNewRequests(): Boolean {
        val memoryInfo = ActivityManager.getMemoryInfo()
        val usageRatio = 1.0f - (memoryInfo.availMem.toFloat() / memoryInfo.totalMem)
        
        return usageRatio > memoryThreshold
    }
    
    fun requestMemoryCleanup() {
        // è§¸ç™¼æ¨¡å‹æ¸…ç†
        modelManager.cleanupUnusedModels()
        
        // å¼·åˆ¶åƒåœ¾å›æ”¶
        System.gc()
    }
}
```

## âš¡ æ•ˆèƒ½æœ€ä½³åŒ–ç­–ç•¥

### æ™ºæ…§é è¼‰å…¥

```kotlin
class SmartPreloader {
    private val usagePatterns = mutableMapOf<String, UsagePattern>()
    
    fun trackUsage(capability: CapabilityType, modelKey: String) {
        val key = "$capability:$modelKey"
        usagePatterns[key] = usagePatterns[key]?.let {
            it.copy(
                usageCount = it.usageCount + 1,
                lastUsed = System.currentTimeMillis()
            )
        } ?: UsagePattern(1, System.currentTimeMillis())
        
        // è§¸ç™¼é æ¸¬æ€§è¼‰å…¥
        schedulePreloadIfNeeded(key)
    }
    
    private fun schedulePreloadIfNeeded(key: String) {
        val pattern = usagePatterns[key] ?: return
        
        if (pattern.usageCount > 3 && pattern.timeSinceLastUse() < TimeUnit.HOURS.toMillis(1)) {
            // é«˜é »ä½¿ç”¨çš„æ¨¡å‹é€²è¡Œé è¼‰å…¥
            preloadModelAsync(key)
        }
    }
}

data class UsagePattern(
    val usageCount: Int,
    val lastUsed: Long
) {
    fun timeSinceLastUse(): Long = System.currentTimeMillis() - lastUsed
}
```

### æ‰¹æ¬¡è™•ç†æœ€ä½³åŒ–

```kotlin
class BatchProcessor {
    private val batchQueue = Channel<InferenceRequest>(capacity = Channel.UNLIMITED)
    private val batchSize = 8
    private val batchTimeout = 50L // ms
    
    init {
        // å•Ÿå‹•æ‰¹æ¬¡è™•ç†å”ç¨‹
        dispatcherScope.launch {
            processBatches()
        }
    }
    
    private suspend fun processBatches() {
        while (true) {
            val batch = collectBatch()
            if (batch.isNotEmpty()) {
                processBatchConcurrently(batch)
            }
        }
    }
    
    private suspend fun collectBatch(): List<InferenceRequest> {
        val batch = mutableListOf<InferenceRequest>()
        
        // ç­‰å¾…ç¬¬ä¸€å€‹è«‹æ±‚
        batch.add(batchQueue.receive())
        
        // æ”¶é›†æ›´å¤šè«‹æ±‚ç›´åˆ°é”åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ™‚
        withTimeoutOrNull(batchTimeout) {
            repeat(batchSize - 1) {
                batch.add(batchQueue.receive())
            }
        }
        
        return batch
    }
}
```

## ğŸ”„ Streaming èˆ‡å³æ™‚è™•ç†

### ä¸²æµè™•ç†æ”¯æ´

```kotlin
class StreamingDispatcher {
    fun dispatchStreamingRequest(
        request: StreamingInferenceRequest,
        onChunk: (InferenceChunk) -> Unit,
        onComplete: () -> Unit,
        onError: (Throwable) -> Unit
    ): Job {
        return dispatcherScope.launch {
            try {
                val runner = selectStreamingRunner(request)
                
                runner.runStream(
                    request,
                    onResult = { chunk ->
                        // ç¢ºä¿åœ¨ä¸»åŸ·è¡Œç·’å›èª¿
                        dispatcherScope.launch(Dispatchers.Main) {
                            onChunk(chunk)
                        }
                    },
                    onComplete = {
                        dispatcherScope.launch(Dispatchers.Main) {
                            onComplete()
                        }
                    },
                    onError = { error ->
                        dispatcherScope.launch(Dispatchers.Main) {
                            onError(error)
                        }
                    }
                )
            } catch (e: Exception) {
                onError(e)
            }
        }
    }
}
```

### å³æ™‚èªéŸ³è™•ç†ç¯„ä¾‹

```kotlin
class RealtimeASRDispatcher {
    private val audioBuffer = Channel<AudioChunk>(capacity = 32)
    
    fun startRealtimeProcessing(
        sessionId: String,
        onTranscript: (String) -> Unit
    ): Job {
        return dispatcherScope.launch {
            val asrRunner = getASRRunner()
            
            // å•Ÿå‹•éŸ³é »è™•ç†å¾ªç’°
            for (audioChunk in audioBuffer) {
                launch {
                    try {
                        val result = asrRunner.processAudioChunk(audioChunk)
                        if (result.isNotEmpty()) {
                            withContext(Dispatchers.Main) {
                                onTranscript(result)
                            }
                        }
                    } catch (e: Exception) {
                        Timber.e(e, "ASR processing failed")
                    }
                }
            }
        }
    }
    
    fun feedAudio(audioData: ByteArray) {
        audioBuffer.trySend(AudioChunk(audioData, System.currentTimeMillis()))
    }
}
```

## ğŸš¨ éŒ¯èª¤è™•ç†èˆ‡é™ç´šç­–ç•¥

### Fallback åŸ·è¡Œæ©Ÿåˆ¶

```kotlin
class FallbackDispatcher {
    suspend fun executeWithFallback(
        request: InferenceRequest,
        maxRetries: Int = 3
    ): InferenceResult {
        val fallbackChain = fallbackPolicy.getFallbackChain(request.capability)
        
        for ((index, runnerSpec) in fallbackChain.withIndex()) {
            try {
                return executeWithRunner(runnerSpec, request)
            } catch (e: Exception) {
                Timber.w(e, "Runner ${runnerSpec.name} failed, attempt ${index + 1}")
                
                // æœ€å¾Œä¸€å€‹é¸é …ä¹Ÿå¤±æ•—ï¼Œé‡æ–°æ‹‹å‡ºéŒ¯èª¤
                if (index == fallbackChain.size - 1) {
                    throw FallbackExhaustedException("All fallback options failed", e)
                }
                
                // çŸ­æš«å»¶é²å†å˜—è©¦ä¸‹ä¸€å€‹é¸é …
                delay(100L * (index + 1))
            }
        }
        
        throw IllegalStateException("Empty fallback chain")
    }
}
```

### è¶…æ™‚èˆ‡å–æ¶ˆè™•ç†

```kotlin
class TimeoutManager {
    private val activeTasks = mutableMapOf<String, Job>()
    
    fun executeWithTimeout(
        request: InferenceRequest,
        timeoutMs: Long = 30_000L
    ): InferenceResult {
        return withTimeout(timeoutMs) {
            val taskId = UUID.randomUUID().toString()
            val job = currentCoroutineContext()[Job]
            
            activeTasks[taskId] = job!!
            
            try {
                executeRequest(request)
            } finally {
                activeTasks.remove(taskId)
            }
        }
    }
    
    fun cancelAllActiveTasks() {
        activeTasks.values.forEach { it.cancel() }
        activeTasks.clear()
    }
}
```

## ğŸ“ˆ ç›£æ§èˆ‡æ•ˆèƒ½æŒ‡æ¨™

### åŸ·è¡Œæ™‚é–“è¿½è¹¤

```kotlin
class PerformanceTracker {
    private val metrics = mutableMapOf<String, MutableList<ExecutionMetrics>>()
    
    suspend fun <T> trackExecution(
        operation: String,
        block: suspend () -> T
    ): T {
        val startTime = System.nanoTime()
        val startMemory = getUsedMemory()
        
        return try {
            block()
        } finally {
            val endTime = System.nanoTime()
            val endMemory = getUsedMemory()
            
            val executionTime = (endTime - startTime) / 1_000_000L // ms
            val memoryDelta = endMemory - startMemory
            
            recordMetrics(operation, ExecutionMetrics(
                executionTimeMs = executionTime,
                memoryDeltaMB = memoryDelta,
                timestamp = System.currentTimeMillis()
            ))
        }
    }
    
    fun getAverageExecutionTime(operation: String): Double {
        return metrics[operation]?.map { it.executionTimeMs }?.average() ?: 0.0
    }
}

data class ExecutionMetrics(
    val executionTimeMs: Long,
    val memoryDeltaMB: Long,
    val timestamp: Long
)
```

## ğŸ›ï¸ èª¿æ ¡èˆ‡è¨­å®šå»ºè­°

### åŸ·è¡Œç·’æ± å¤§å°èª¿æ ¡

| è£ç½®é¡å‹ | CPU Cores | å»ºè­°é…ç½® |
|----------|-----------|----------|
| **ä½éšè£ç½®** | 4 cores | IO: 2, Default: 2, Inference: 1 |
| **ä¸­éšè£ç½®** | 6-8 cores | IO: 3, Default: 4, Inference: 2 |
| **é«˜éšè£ç½®** | 8+ cores | IO: 4, Default: 6, Inference: 3 |

### ä¸¦ç™¼é™åˆ¶è¨­å®š

```kotlin
object DispatcherTuning {
    fun getOptimalConfiguration(deviceInfo: DeviceInfo): DispatcherConfig {
        return when {
            deviceInfo.totalMemoryMB < 4096 -> DispatcherConfig(
                maxConcurrentRequests = 2,
                maxConcurrentModelLoading = 1,
                batchSize = 4
            )
            deviceInfo.totalMemoryMB < 8192 -> DispatcherConfig(
                maxConcurrentRequests = 4,
                maxConcurrentModelLoading = 2,
                batchSize = 8
            )
            else -> DispatcherConfig(
                maxConcurrentRequests = 6,
                maxConcurrentModelLoading = 3,
                batchSize = 12
            )
        }
    }
}
```

## ğŸ”— ç›¸é—œç« ç¯€

- **Runner ä»‹é¢**: [Runner çµ±ä¸€ä»‹é¢](../02-Interfaces/runner-interface.md) - Runner åŸ·è¡Œè¦ç¯„
- **éŒ¯èª¤è™•ç†**: [éŒ¯èª¤ç¢¼å®šç¾©](../05-Error-Handling/error-codes.md) - Fallback ç›¸é—œéŒ¯èª¤
- **æ¨¡å‹ç®¡ç†**: [æ¨¡å‹ç¯„åœç­–ç•¥](../03-Models/model-scope.md) - åŸ·è¡Œç·’å®‰å…¨è€ƒé‡
- **æ•ˆèƒ½æœ€ä½³åŒ–**: [æ•ˆèƒ½èª¿å„ªæŒ‡å—](./performance-optimization.md) - ç³»çµ±æ•ˆèƒ½å„ªåŒ–

## ğŸ’¡ æœ€ä½³å¯¦å‹™ç¸½çµ

### âœ… æ¨è–¦åšæ³•

- **ç•°æ­¥å„ªå…ˆ**: æ‰€æœ‰é•·æ™‚é–“æ“ä½œä½¿ç”¨ Coroutines
- **é©åº¦ä¸¦ç™¼**: æ ¹æ“šè£ç½®èƒ½åŠ›è¨­å®šåˆç†çš„ä¸¦ç™¼é™åˆ¶
- **æ™ºæ…§é è¼‰**: åŸºæ–¼ä½¿ç”¨æ¨¡å¼é€²è¡Œé æ¸¬æ€§æ¨¡å‹è¼‰å…¥
- **éŒ¯èª¤è™•ç†**: å¯¦ä½œå®Œæ•´çš„ Fallback èˆ‡é‡è©¦æ©Ÿåˆ¶
- **ç›£æ§è¿½è¹¤**: è¨˜éŒ„åŸ·è¡Œæ™‚é–“èˆ‡è³‡æºä½¿ç”¨æƒ…æ³

### ğŸš« é¿å…çš„é™·é˜±

- **ä¸»åŸ·è¡Œç·’é˜»å¡**: çµ•ä¸åœ¨ UI åŸ·è¡Œç·’é€²è¡Œæ¨è«–æ“ä½œ
- **éåº¦ä¸¦ç™¼**: é¿å…è¶…å‡ºè£ç½®èƒ½åŠ›çš„ä¸¦ç™¼æ•¸é‡
- **è³‡æºæ´©æ¼**: ç¢ºä¿æ­£ç¢ºé‡‹æ”¾æ¨¡å‹èˆ‡åŸ·è¡Œç·’è³‡æº
- **å¿½ç•¥è¶…æ™‚**: æ‰€æœ‰æ¨è«–æ“ä½œéƒ½æ‡‰è¨­å®šåˆç†è¶…æ™‚
- **å¿½ç•¥è¨˜æ†¶é«”å£“åŠ›**: åœ¨è¨˜æ†¶é«”ä¸è¶³æ™‚æ‡‰ä¸»å‹•é™ç´šæˆ–æ‹’çµ•è«‹æ±‚

---

ğŸ“ **è¿”å›**: [Runtime é¦–é ](./README.md) | **ä¸‹ä¸€ç¯‡**: [æ•ˆèƒ½æœ€ä½³åŒ–æŒ‡å—](./performance-optimization.md) 